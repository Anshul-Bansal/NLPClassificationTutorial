{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterDetector:\n",
    "    \n",
    "    def __init__(self, bert_layer, max_seq_length=128, lr=0.0001, epochs=15, batch_size=32):\n",
    "        \n",
    "        # BERT and Tokenization params\n",
    "        self.bert_layer = bert_layer\n",
    "        \n",
    "        self.max_seq_length = max_seq_length        \n",
    "        vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "        do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()\n",
    "        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "        \n",
    "        # Learning control params\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.models = []\n",
    "        self.scores = {}\n",
    "        \n",
    "        \n",
    "    def encode(self, texts):\n",
    "                \n",
    "        all_tokens = []\n",
    "        all_masks = []\n",
    "        all_segments = []\n",
    "\n",
    "        for text in texts:\n",
    "            text = self.tokenizer.tokenize(text)\n",
    "            text = text[:self.max_seq_length - 2]\n",
    "            input_sequence = ['[CLS]'] + text + ['[SEP]']\n",
    "            pad_len = self.max_seq_length - len(input_sequence)\n",
    "\n",
    "            tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "            tokens += [0] * pad_len\n",
    "            pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "            segment_ids = [0] * self.max_seq_length\n",
    "\n",
    "            all_tokens.append(tokens)\n",
    "            all_masks.append(pad_masks)\n",
    "            all_segments.append(segment_ids)\n",
    "\n",
    "        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "    \n",
    "    \n",
    "    def build_model(self):\n",
    "        \n",
    "        input_word_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "        input_mask = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "        segment_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='segment_ids')\n",
    "        \n",
    "        \"\"\"@Doubt\n",
    "        what is pooled output and sequence output\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   \n",
    "        clf_output = sequence_output[:, 0, :]\n",
    "        out = Dense(1, activation='sigmoid')(clf_output)\n",
    "        \n",
    "        model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "        optimizer = SGD(learning_rate=self.lr, momentum=0.8)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def train(self, X):\n",
    "        \n",
    "        for fold, (trn_idx, val_idx) in enumerate(skf.split(X['text_cleaned'], X['keyword'])):\n",
    "            \n",
    "            print('\\nFold {}\\n'.format(fold))\n",
    "        \n",
    "            X_trn_encoded = self.encode(X.loc[trn_idx, 'text_cleaned'].str.lower())\n",
    "            y_trn = X.loc[trn_idx, 'target_relabeled']\n",
    "            X_val_encoded = self.encode(X.loc[val_idx, 'text_cleaned'].str.lower())\n",
    "            y_val = X.loc[val_idx, 'target_relabeled']\n",
    "        \n",
    "            # Callbacks\n",
    "            metrics = ClassificationReport(train_data=(X_trn_encoded, y_trn), validation_data=(X_val_encoded, y_val))\n",
    "            \n",
    "            # Model\n",
    "            model = self.build_model()        \n",
    "            model.fit(X_trn_encoded, y_trn, validation_data=(X_val_encoded, y_val), callbacks=[metrics], epochs=self.epochs, batch_size=self.batch_size)\n",
    "            \n",
    "            self.models.append(model)\n",
    "            self.scores[fold] = {\n",
    "                'train': {\n",
    "                    'precision': metrics.train_precision_scores,\n",
    "                    'recall': metrics.train_recall_scores,\n",
    "                    'f1': metrics.train_f1_scores                    \n",
    "                },\n",
    "                'validation': {\n",
    "                    'precision': metrics.val_precision_scores,\n",
    "                    'recall': metrics.val_recall_scores,\n",
    "                    'f1': metrics.val_f1_scores                    \n",
    "                }\n",
    "            }\n",
    "                    \n",
    "                \n",
    "    def plot_learning_curve(self):\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=K, ncols=2, figsize=(20, K * 6), dpi=100)\n",
    "    \n",
    "        for i in range(K):\n",
    "            \n",
    "            # Classification Report curve\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[i].history.history['val_accuracy'], ax=axes[i][0], label='val_accuracy')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['precision'], ax=axes[i][0], label='val_precision')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['recall'], ax=axes[i][0], label='val_recall')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['f1'], ax=axes[i][0], label='val_f1')        \n",
    "\n",
    "            axes[i][0].legend() \n",
    "            axes[i][0].set_title('Fold {} Validation Classification Report'.format(i), fontsize=14)\n",
    "\n",
    "            # Loss curve\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['loss'], ax=axes[i][1], label='train_loss')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['val_loss'], ax=axes[i][1], label='val_loss')\n",
    "\n",
    "            axes[i][1].legend() \n",
    "            axes[i][1].set_title('Fold {} Train / Validation Loss'.format(i), fontsize=14)\n",
    "\n",
    "            for j in range(2):\n",
    "                axes[i][j].set_xlabel('Epoch', size=12)\n",
    "                axes[i][j].tick_params(axis='x', labelsize=12)\n",
    "                axes[i][j].tick_params(axis='y', labelsize=12)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X_test_encoded = self.encode(X['text_cleaned'].str.lower())\n",
    "        y_pred = np.zeros((X_test_encoded[0].shape[0], 1))\n",
    "\n",
    "        for model in self.models:\n",
    "            y_pred += model.predict(X_test_encoded) / len(self.models)\n",
    "\n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
