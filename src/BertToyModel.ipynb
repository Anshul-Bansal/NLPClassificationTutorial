{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanText\n",
    "from tensorflowEval import ClassificationReport\n",
    "from DisasterDetector import DisasterDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/raw/train.csv\",dtype={'id': np.int16, 'target': np.int8})\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this  # earthquake...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Forest fire near La Ronge Sask .  Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>All residents asked to  ' shelter in place '  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>13,000 people receive  # wildfires evacuation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Just got sent this photo from Ruby  # Alaska a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       text_cleaned  target\n",
       "0   1  Our Deeds are the Reason of this  # earthquake...       1\n",
       "1   4           Forest fire near La Ronge Sask .  Canada       1\n",
       "2   5  All residents asked to  ' shelter in place '  ...       1\n",
       "3   6  13,000 people receive  # wildfires evacuation ...       1\n",
       "4   7  Just got sent this photo from Ruby  # Alaska a...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train['text_cleaned'] = df_train['text'].apply(lambda s : cleanText.clean(s))\n",
    "df_train = pd.read_csv(\"../data/raw/trainProc.csv\",dtype={'id': np.int16, 'target': np.int8})\n",
    "#df_train[[\"id\",\"text_cleaned\",\"target\"]].to_csv(\"../data/raw/trainProc.csv\",index=False)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "seed = 100\n",
    "skf = StratifiedKFold(n_splits=K, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 3.11 s, total: 14.6 s\n",
      "Wall time: 52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "\n",
      "Epoch 1/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.6842 - accuracy: 0.5888\n",
      "Epoch: 1 - Training Precision: 0.737729 - Training Recall: 0.684738 - Training F1: 0.684214\n",
      "Epoch: 1 - Validation Precision: 0.713395 - Validation Recall: 0.666675 - Validation F1: 0.664692\n",
      "119/119 [==============================] - 701s 6s/step - loss: 0.6842 - accuracy: 0.5888 - val_loss: 0.6146 - val_accuracy: 0.6974\n",
      "Epoch 2/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.5633 - accuracy: 0.7488\n",
      "Epoch: 2 - Training Precision: 0.765834 - Training Recall: 0.756316 - Training F1: 0.759263\n",
      "Epoch: 2 - Validation Precision: 0.743897 - Validation Recall: 0.733994 - Validation F1: 0.736788\n",
      "119/119 [==============================] - 907s 8s/step - loss: 0.5633 - accuracy: 0.7488 - val_loss: 0.5391 - val_accuracy: 0.7465\n",
      "Epoch 3/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.4944 - accuracy: 0.7801\n",
      "Epoch: 3 - Training Precision: 0.793898 - Training Recall: 0.7851 - Training F1: 0.788096\n",
      "Epoch: 3 - Validation Precision: 0.772456 - Validation Recall: 0.764893 - Validation F1: 0.76747\n",
      "119/119 [==============================] - 1265s 11s/step - loss: 0.4944 - accuracy: 0.7801 - val_loss: 0.4940 - val_accuracy: 0.7749\n",
      "Epoch 4/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.8011\n",
      "Epoch: 4 - Training Precision: 0.811968 - Training Recall: 0.801878 - Training F1: 0.80529\n",
      "Epoch: 4 - Validation Precision: 0.790157 - Validation Recall: 0.781967 - Validation F1: 0.784793\n",
      "119/119 [==============================] - 955s 8s/step - loss: 0.4537 - accuracy: 0.8011 - val_loss: 0.4700 - val_accuracy: 0.7917\n",
      "Epoch 5/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.8106\n",
      "Epoch: 5 - Training Precision: 0.82232 - Training Recall: 0.812748 - Training F1: 0.816096\n",
      "Epoch: 5 - Validation Precision: 0.796828 - Validation Recall: 0.789537 - Validation F1: 0.792168\n",
      "119/119 [==============================] - 941s 8s/step - loss: 0.4283 - accuracy: 0.8106 - val_loss: 0.4553 - val_accuracy: 0.7985\n",
      "Epoch 6/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.4086 - accuracy: 0.8221\n",
      "Epoch: 6 - Training Precision: 0.835088 - Training Recall: 0.819295 - Training F1: 0.824098\n",
      "Epoch: 6 - Validation Precision: 0.804962 - Validation Recall: 0.79226 - Validation F1: 0.796164\n",
      "119/119 [==============================] - 764s 6s/step - loss: 0.4086 - accuracy: 0.8221 - val_loss: 0.4458 - val_accuracy: 0.8038\n",
      "Epoch 7/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.3930 - accuracy: 0.8284\n",
      "Epoch: 7 - Training Precision: 0.848044 - Training Recall: 0.824007 - Training F1: 0.830324\n",
      "Epoch: 7 - Validation Precision: 0.815008 - Validation Recall: 0.795444 - Validation F1: 0.800578\n",
      "119/119 [==============================] - 745s 6s/step - loss: 0.3930 - accuracy: 0.8284 - val_loss: 0.4408 - val_accuracy: 0.8096\n",
      "Epoch 8/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.3790 - accuracy: 0.8379\n",
      "Epoch: 8 - Training Precision: 0.844102 - Training Recall: 0.832973 - Training F1: 0.836835\n",
      "Epoch: 8 - Validation Precision: 0.809707 - Validation Recall: 0.802169 - Validation F1: 0.804922\n",
      "119/119 [==============================] - 704s 6s/step - loss: 0.3790 - accuracy: 0.8379 - val_loss: 0.4330 - val_accuracy: 0.8109\n",
      "Epoch 9/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.3661 - accuracy: 0.8426\n",
      "Epoch: 9 - Training Precision: 0.853232 - Training Recall: 0.838584 - Training F1: 0.843327\n",
      "Epoch: 9 - Validation Precision: 0.817137 - Validation Recall: 0.805645 - Validation F1: 0.809405\n",
      "119/119 [==============================] - 756s 6s/step - loss: 0.3661 - accuracy: 0.8426 - val_loss: 0.4291 - val_accuracy: 0.8161\n",
      "Epoch 10/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.3541 - accuracy: 0.8471\n",
      "Epoch: 10 - Training Precision: 0.855376 - Training Recall: 0.848033 - Training F1: 0.850893\n",
      "Epoch: 10 - Validation Precision: 0.809552 - Validation Recall: 0.804898 - Validation F1: 0.806784\n",
      "119/119 [==============================] - 654s 5s/step - loss: 0.3541 - accuracy: 0.8471 - val_loss: 0.4286 - val_accuracy: 0.8119\n",
      "\n",
      "Fold 1\n",
      "\n",
      "Epoch 1/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.5317 - accuracy: 0.7373\n",
      "Epoch: 1 - Training Precision: 0.801255 - Training Recall: 0.792611 - Training F1: 0.795609\n",
      "Epoch: 1 - Validation Precision: 0.811533 - Validation Recall: 0.804004 - Validation F1: 0.806765\n",
      "119/119 [==============================] - 672s 6s/step - loss: 0.5317 - accuracy: 0.7373 - val_loss: 0.4315 - val_accuracy: 0.8127\n",
      "Epoch 2/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.4384 - accuracy: 0.8072\n",
      "Epoch: 2 - Training Precision: 0.82976 - Training Recall: 0.815234 - Training F1: 0.819732\n",
      "Epoch: 2 - Validation Precision: 0.829693 - Validation Recall: 0.814696 - Validation F1: 0.819291\n",
      "119/119 [==============================] - 654s 5s/step - loss: 0.4384 - accuracy: 0.8072 - val_loss: 0.4039 - val_accuracy: 0.8263\n",
      "Epoch 3/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.4142 - accuracy: 0.8266\n",
      "Epoch: 3 - Training Precision: 0.830251 - Training Recall: 0.823443 - Training F1: 0.826061\n",
      "Epoch: 3 - Validation Precision: 0.829374 - Validation Recall: 0.821998 - Validation F1: 0.824784\n",
      "119/119 [==============================] - 867s 7s/step - loss: 0.4142 - accuracy: 0.8266 - val_loss: 0.3935 - val_accuracy: 0.8300\n",
      "Epoch 4/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.3981 - accuracy: 0.8361\n",
      "Epoch: 4 - Training Precision: 0.8493 - Training Recall: 0.831405 - Training F1: 0.836751\n",
      "Epoch: 4 - Validation Precision: 0.84397 - Validation Recall: 0.823607 - Validation F1: 0.829314\n",
      "119/119 [==============================] - 861s 7s/step - loss: 0.3981 - accuracy: 0.8361 - val_loss: 0.3884 - val_accuracy: 0.8368\n",
      "Epoch 5/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.3847 - accuracy: 0.8411\n",
      "Epoch: 5 - Training Precision: 0.841083 - Training Recall: 0.834843 - Training F1: 0.837316\n",
      "Epoch: 5 - Validation Precision: 0.832214 - Validation Recall: 0.825135 - Validation F1: 0.827843\n",
      "119/119 [==============================] - 919s 8s/step - loss: 0.3847 - accuracy: 0.8411 - val_loss: 0.3852 - val_accuracy: 0.8329\n",
      "Epoch 6/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.3755 - accuracy: 0.8461\n",
      "Epoch: 6 - Training Precision: 0.853794 - Training Recall: 0.842461 - Training F1: 0.846427\n",
      "Epoch: 6 - Validation Precision: 0.838315 - Validation Recall: 0.82586 - Validation F1: 0.829998\n",
      "119/119 [==============================] - 796s 7s/step - loss: 0.3755 - accuracy: 0.8461 - val_loss: 0.3814 - val_accuracy: 0.8360\n",
      "Epoch 7/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.8511\n",
      "Epoch: 7 - Training Precision: 0.861005 - Training Recall: 0.845778 - Training F1: 0.850707\n",
      "Epoch: 7 - Validation Precision: 0.844653 - Validation Recall: 0.828409 - Validation F1: 0.833399\n",
      "119/119 [==============================] - 667s 6s/step - loss: 0.3636 - accuracy: 0.8511 - val_loss: 0.3787 - val_accuracy: 0.8400\n",
      "Epoch 8/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.3547 - accuracy: 0.8563\n",
      "Epoch: 8 - Training Precision: 0.861305 - Training Recall: 0.849202 - Training F1: 0.853406\n",
      "Epoch: 8 - Validation Precision: 0.841427 - Validation Recall: 0.828771 - Validation F1: 0.832978\n",
      "119/119 [==============================] - 669s 6s/step - loss: 0.3547 - accuracy: 0.8563 - val_loss: 0.3782 - val_accuracy: 0.8389\n",
      "Epoch 9/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.3449 - accuracy: 0.8597\n",
      "Epoch: 9 - Training Precision: 0.869308 - Training Recall: 0.853436 - Training F1: 0.858577\n",
      "Epoch: 9 - Validation Precision: 0.846928 - Validation Recall: 0.830096 - Validation F1: 0.835224\n",
      "119/119 [==============================] - 672s 6s/step - loss: 0.3449 - accuracy: 0.8597 - val_loss: 0.3773 - val_accuracy: 0.8418\n",
      "Epoch 10/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.3370 - accuracy: 0.8616\n",
      "Epoch: 10 - Training Precision: 0.873406 - Training Recall: 0.856656 - Training F1: 0.862023\n",
      "Epoch: 10 - Validation Precision: 0.847212 - Validation Recall: 0.829489 - Validation F1: 0.834786\n",
      "119/119 [==============================] - 666s 6s/step - loss: 0.3370 - accuracy: 0.8616 - val_loss: 0.3777 - val_accuracy: 0.8416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21h 7min 19s, sys: 2h 36min 55s, total: 23h 44min 14s\n",
      "Wall time: 4h 25min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = DisasterDetector(bert_layer,skf, max_seq_length=27,lr=0.0001, \n",
    "                       epochs=10, \n",
    "                       batch_size=32)\n",
    "\n",
    "clf.train(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore details of BERT layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization import load_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "blVocabFile = bert_layer.resolved_object.vocab_file.asset_path.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "properVocab = load_vocab(blVocabFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(properVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenInBert = list(properVocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenInBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the necessitated\n",
      "Number of english tokens 27615\n"
     ]
    }
   ],
   "source": [
    "#english tonkens ranges from\n",
    "start = 1996\n",
    "end = 29611\n",
    "print(tokenInBert[start],tokenInBert[end])\n",
    "print(\"Number of english tokens\",(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused195]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenInBert[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'odict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c7239ae2dc68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproperVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'odict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "properVocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.81188"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(properVocab)/10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sys.getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(df_train.text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization import FullTokenizer\n",
    "tokenizer = FullTokenizer(blVocabFile, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Our Deeds are the,Reason of this  # earthquake May ALLAH Forgive us all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " ',',\n",
       " 'reason',\n",
       " 'of',\n",
       " 'this',\n",
       " '#',\n",
       " 'earthquake',\n",
       " 'may',\n",
       " 'allah',\n",
       " 'forgive',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = tokenizer.tokenize(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ar', '##pan']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"arpan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " ',',\n",
       " 'reason',\n",
       " 'of',\n",
       " 'this',\n",
       " '#',\n",
       " 'earthquake',\n",
       " 'may',\n",
       " 'allah',\n",
       " 'forgive',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 27\n",
    "text = text[:max_seq_length - 2]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'our',\n",
       " 'deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " ',',\n",
       " 'reason',\n",
       " 'of',\n",
       " 'this',\n",
       " '#',\n",
       " 'earthquake',\n",
       " 'may',\n",
       " 'allah',\n",
       " 'forgive',\n",
       " 'us',\n",
       " 'all',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence = ['[CLS]'] + text + ['[SEP]']\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1014\n",
      "1 1015\n",
      "2 1016\n",
      "3 1017\n",
      "4 1018\n",
      "5 1019\n",
      "6 1020\n",
      "7 1021\n",
      "8 1022\n",
      "9 1023\n",
      "10 2184\n",
      "11 2340\n",
      "12 2260\n",
      "13 2410\n",
      "14 2403\n",
      "15 2321\n",
      "16 2385\n",
      "17 2459\n",
      "18 2324\n",
      "19 2539\n",
      "20 2322\n",
      "21 2538\n",
      "22 2570\n",
      "23 2603\n",
      "24 2484\n",
      "25 2423\n",
      "26 2656\n",
      "27 2676\n",
      "28 2654\n",
      "29 2756\n",
      "30 2382\n",
      "31 2861\n",
      "32 3590\n",
      "33 3943\n",
      "34 4090\n",
      "35 3486\n",
      "36 4029\n",
      "37 4261\n",
      "38 4229\n",
      "39 4464\n",
      "40 2871\n",
      "41 4601\n",
      "42 4413\n",
      "43 4724\n",
      "44 4008\n",
      "45 3429\n",
      "46 4805\n",
      "47 4700\n",
      "48 4466\n",
      "49 4749\n",
      "50 2753\n",
      "51 4868\n",
      "52 4720\n",
      "53 5187\n",
      "54 5139\n",
      "55 4583\n",
      "56 5179\n",
      "57 5401\n",
      "58 5388\n",
      "59 5354\n",
      "60 3438\n",
      "61 6079\n",
      "62 5786\n",
      "63 6191\n",
      "64 4185\n",
      "65 3515\n",
      "66 5764\n",
      "67 6163\n",
      "68 6273\n",
      "69 6353\n",
      "70 3963\n",
      "71 6390\n",
      "72 5824\n",
      "73 6421\n",
      "74 6356\n",
      "75 4293\n",
      "76 6146\n",
      "77 6255\n",
      "78 6275\n",
      "79 6535\n",
      "80 3770\n",
      "81 6282\n",
      "82 6445\n",
      "83 6640\n",
      "84 6391\n",
      "85 5594\n",
      "86 6564\n",
      "87 6584\n",
      "88 6070\n",
      "89 6486\n",
      "90 3938\n",
      "91 6205\n",
      "92 6227\n",
      "93 6109\n",
      "94 6365\n",
      "95 5345\n",
      "96 5986\n",
      "97 5989\n",
      "98 5818\n",
      "99 5585\n",
      "100 2531\n",
      "101 7886\n",
      "102 9402\n",
      "103 9800\n",
      "104 9645\n",
      "105 8746\n",
      "106 10114\n",
      "107 10550\n",
      "108 10715\n",
      "109 11518\n",
      "110 7287\n",
      "111 11118\n",
      "112 11176\n",
      "113 12104\n",
      "114 12457\n",
      "115 10630\n",
      "116 12904\n",
      "117 12567\n",
      "118 12963\n",
      "119 13285\n",
      "120 6036\n",
      "121 12606\n",
      "122 13092\n",
      "123 13138\n",
      "124 13412\n",
      "125 8732\n",
      "126 14010\n",
      "127 13029\n",
      "128 11899\n",
      "129 14378\n",
      "130 7558\n",
      "131 14677\n",
      "132 14078\n",
      "133 14506\n",
      "134 15170\n",
      "135 11502\n",
      "136 15407\n",
      "137 14989\n",
      "138 15028\n",
      "139 16621\n",
      "140 8574\n",
      "141 15471\n",
      "142 16087\n",
      "143 16065\n",
      "144 14748\n",
      "145 13741\n",
      "146 16333\n",
      "147 16471\n",
      "148 16459\n",
      "149 17332\n",
      "150 5018\n",
      "151 16528\n",
      "152 15017\n",
      "153 16710\n",
      "154 16666\n",
      "155 14168\n",
      "156 16734\n",
      "157 17403\n",
      "158 17696\n",
      "159 18914\n",
      "160 8148\n",
      "161 17365\n",
      "162 17832\n",
      "163 17867\n",
      "164 17943\n",
      "165 13913\n",
      "166 18610\n",
      "167 16785\n",
      "168 16923\n",
      "169 18582\n",
      "170 10894\n",
      "171 18225\n",
      "172 18253\n",
      "173 19410\n",
      "174 19492\n",
      "175 12862\n",
      "176 18561\n",
      "177 18118\n",
      "178 19289\n",
      "179 20311\n",
      "180 8380\n",
      "181 18596\n",
      "182 17691\n",
      "183 18677\n",
      "184 19681\n",
      "185 15376\n",
      "186 19609\n",
      "187 19446\n",
      "188 19121\n",
      "189 20500\n",
      "190 11827\n",
      "191 19871\n",
      "192 17613\n",
      "193 19984\n",
      "194 19955\n",
      "195 17317\n",
      "196 20035\n",
      "197 19975\n",
      "198 20003\n",
      "199 20713\n",
      "200 3263\n",
      "201 16345\n",
      "202 16798\n",
      "203 18540\n",
      "204 19627\n",
      "205 16327\n",
      "206 18744\n",
      "207 19843\n",
      "208 18512\n",
      "209 19348\n",
      "210 12875\n",
      "211 19235\n",
      "212 18164\n",
      "213 19883\n",
      "214 19936\n",
      "215 17405\n",
      "216 20294\n",
      "217 20335\n",
      "218 20741\n",
      "219 20636\n",
      "220 10545\n",
      "221 19594\n",
      "222 19015\n",
      "223 20802\n",
      "224 19711\n",
      "225 14993\n",
      "226 21035\n",
      "227 21489\n",
      "228 22238\n",
      "229 22777\n",
      "230 11816\n",
      "231 20304\n",
      "232 20666\n",
      "233 22115\n",
      "234 22018\n",
      "235 17825\n",
      "236 23593\n",
      "237 23297\n",
      "238 22030\n",
      "239 23688\n",
      "240 11212\n",
      "241 22343\n",
      "242 22431\n",
      "243 22884\n",
      "244 24194\n",
      "245 21005\n",
      "246 22376\n",
      "247 23380\n",
      "248 24568\n",
      "249 23628\n",
      "250 5539\n",
      "251 22582\n",
      "252 22898\n",
      "253 23254\n",
      "254 22234\n",
      "255 20637\n",
      "256 17273\n",
      "257 24368\n",
      "258 24398\n",
      "259 25191\n",
      "260 13539\n",
      "261 24441\n",
      "262 21950\n",
      "263 25246\n",
      "264 21611\n",
      "265 20549\n",
      "266 25162\n",
      "267 25491\n",
      "268 25143\n",
      "269 25717\n",
      "270 13756\n",
      "271 25103\n",
      "272 24231\n",
      "273 25371\n",
      "274 25586\n",
      "275 17528\n",
      "276 25113\n",
      "277 25578\n",
      "278 24709\n",
      "279 25745\n",
      "280 13427\n",
      "281 22955\n",
      "282 26267\n",
      "283 25504\n",
      "284 26871\n",
      "285 21777\n",
      "286 24921\n",
      "287 23090\n",
      "288 24841\n",
      "289 27054\n",
      "290 17222\n",
      "291 27173\n",
      "292 25797\n",
      "293 26953\n",
      "294 28135\n",
      "295 21679\n",
      "296 27200\n",
      "297 27502\n",
      "298 27240\n",
      "299 25926\n",
      "300 3998\n",
      "301 19123\n",
      "302 22060\n",
      "303 19988\n",
      "304 23859\n",
      "305 20405\n",
      "306 24622\n",
      "307 24559\n",
      "308 24232\n",
      "309 25048\n",
      "310 17196\n",
      "311 23532\n",
      "312 21036\n",
      "313 22997\n",
      "314 26257\n",
      "315 22904\n",
      "316 23980\n",
      "317 26628\n",
      "318 27003\n",
      "319 26499\n",
      "320 13710\n",
      "321 24030\n",
      "322 23768\n",
      "323 25392\n",
      "324 27234\n",
      "325 19652\n",
      "326 28188\n",
      "327 28469\n",
      "328 25256\n",
      "329 29567\n",
      "330 14210\n",
      "331 27533\n",
      "332 29327\n",
      "333 21211\n",
      "334 29562\n",
      "335 24426\n",
      "336 27954\n",
      "337 28489\n",
      "338 27908\n",
      "339 28977\n",
      "340 16029\n",
      "341 28358\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'342'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-89c049b316e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproperVocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '342'"
     ]
    }
   ],
   "source": [
    "for x in range(0,1000):\n",
    "    print(x,properVocab[str(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1037\n",
      "b 1038\n",
      "c 1039\n",
      "d 1040\n",
      "e 1041\n",
      "f 1042\n",
      "g 1043\n",
      "h 1044\n",
      "i 1045\n",
      "j 1046\n",
      "k 1047\n",
      "l 1048\n",
      "m 1049\n",
      "n 1050\n",
      "o 1051\n",
      "p 1052\n",
      "q 1053\n",
      "r 1054\n",
      "s 1055\n",
      "t 1056\n",
      "u 1057\n",
      "v 1058\n",
      "w 1059\n",
      "x 1060\n",
      "y 1061\n",
      "z 1062\n"
     ]
    }
   ],
   "source": [
    "for x in range(97,123):\n",
    "    print(chr(x),properVocab[chr(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7632, 2032]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids([\"hi\",\"him\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2256,\n",
       " 15616,\n",
       " 2024,\n",
       " 1996,\n",
       " 1010,\n",
       " 3114,\n",
       " 1997,\n",
       " 2023,\n",
       " 1001,\n",
       " 8372,\n",
       " 2089,\n",
       " 16455,\n",
       " 9641,\n",
       " 2149,\n",
       " 2035,\n",
       " 102]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "pad_len = self.max_seq_length - len(input_sequence)\n",
    "tokens += [0] * pad_len\n",
    "pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "segment_ids = [0] * self.max_seq_length\n",
    "\n",
    "all_tokens.append(tokens)\n",
    "all_masks.append(pad_masks)\n",
    "all_segments.append(segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "all_masks = []\n",
    "all_segments = []\n",
    "\n",
    "for text in texts:\n",
    "    text = self.tokenizer.tokenize(text)\n",
    "    text = text[:self.max_seq_length - 2]\n",
    "    input_sequence = ['[CLS]'] + text + ['[SEP]']\n",
    "    pad_len = self.max_seq_length - len(input_sequence)\n",
    "\n",
    "    tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "    tokens += [0] * pad_len\n",
    "    pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "    segment_ids = [0] * self.max_seq_length\n",
    "\n",
    "    all_tokens.append(tokens)\n",
    "    all_masks.append(pad_masks)\n",
    "    all_segments.append(segment_ids)\n",
    "\n",
    "return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
